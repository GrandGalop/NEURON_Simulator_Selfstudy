{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bade2ec9",
   "metadata": {},
   "source": [
    "# Model neuron simulation\n",
    "## An analysis of learning performance changes in spiking neural networks(SNN)\n",
    "### 김용주, 김태호\n",
    "\n",
    "* HH model cell을 실험 조건에 맞게 배치해 볼 것\n",
    "    * Ball And Stick Tutorial에서는 z축을 중심으로 2Pi/n rad마다 배치하였음.\n",
    "    * 해당 실험에서는 n by n의 excitatory neurons를 z=0에 배치, n by n (n=10, 15, 20)의 inhibitory neurons를 z=-1에 배치해 보고자 함\n",
    "    * excitatory neuron -> inhibitory neurons는 x 좌표와 y좌표가 둘 다 맞을 때 연결\n",
    "    * inhibitory neuron -> excitatory neurons는 x 좌표와 y좌표가 둘 다 맞을 때를 제외하고 연결\n",
    "\n",
    "* Q1 : input stimulation은 어떻게 구현할 수 있을까?\n",
    "    * Convolutional한 MNIST 데이터셋으로 어떻게 excitatory neurons을 흥분시킬 수 있는가? -> NetCon을 이용하여 Value 에서 Frequency로 바꾸는 작업이 필요할 듯\n",
    "* Q2 : 신경망의 가중치 설정 및 조작\n",
    "    * CNN perceptron -> SNN neuron\n",
    "        * Weight -> Synaptic Weight (Weight term)\n",
    "        * Transfer function 에 대응되는 개념이 SNN에는 존재하지 않는가?\n",
    "        * Refractory는 불변의 scale인가?\n",
    "* Q3 : model image의 learning을 어떤 식으로 진행시킬 수 있을까?\n",
    "    * STDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df943109",
   "metadata": {},
   "source": [
    "## Class 선언\n",
    "\n",
    "### Cell class\n",
    "* neuron의 기본적인 position, 구성요소 등을 정의하는 함수\n",
    "* 후술할 ExcitatoryCell과 InhibitoryCell에서 define되는 _setup_morphology()와 _setup_biophysics() 를 __init__에서 recall함으로써 neuron을 define함\n",
    "\n",
    "### ExcitatoryCell\n",
    "* 흥분성 뉴런을 정의하는 class\n",
    "    * 휴지기 전압 : -65mV\n",
    "    * 초기화 전압 : -65mV\n",
    "    * Threshold : -52mV\n",
    "    * Refractory period : 5ms\n",
    "    \n",
    "### InhibitoryCell\n",
    "* 억제성 뉴런을 정의하는 class\n",
    "    * 휴지기 전압 : -60mV\n",
    "    * 초기화 전압 : -45mV\n",
    "    * Threshold : -40mV\n",
    "    * Refractory period : 2ms\n",
    "    \n",
    "* Refractory period는 설정 완료\n",
    "    * 어떻게 휴지기 전압, 초기화 전압, Threshold를 설정 가능한가?\n",
    "        * 휴지기 전압, 초기화 전압의 차이점은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e20d6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuron import h, gui\n",
    "from neuron.units import ms, mV\n",
    "h.load_file('stdrun.hoc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d014a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron import h\n",
    "class Cell:\n",
    "    def __init__(self, gid, x, y, z):\n",
    "        self._gid = gid\n",
    "        self._setup_morphology()\n",
    "        self.all = self.soma.wholetree()\n",
    "        self._setup_biophysics()\n",
    "        self.x = self.y = self.z = 0\n",
    "        h.define_shape()\n",
    "        self._set_position(x, y, z)\n",
    "        self._spike_detector = h.NetCon(self.soma(0.5)._ref_v, None, sec=self.soma)\n",
    "        self.spike_times = h.Vector()\n",
    "        self._spike_detector.record(self.spike_times)\n",
    "        self._ncs = []\n",
    "        self.soma_v = h.Vector().record(self.soma(0.5)._ref_v)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}[{}]'.format(self.name, self._gid)\n",
    "    \n",
    "    def _set_position(self, x, y, z):\n",
    "        for sec in self.all:\n",
    "            for i in range(sec.n3d()):\n",
    "                sec.pt3dchange(i, x - self.x + sec.x3d(i), y - self.y + sec.y3d(i), z - self.z + sec.z3d(i), sec.diam3d(i))\n",
    "        self.x, self.y, self.z = x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e772f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcitatoryCell(Cell):\n",
    "    name = 'ExcitatoryCell'\n",
    "    \n",
    "    def _setup_morphology(self):\n",
    "        self.soma = h.Section(name='soma', cell=self)\n",
    "        self.dend = h.Section(name='dend', cell=self)\n",
    "        self.dend.connect(self.soma)\n",
    "        self.soma.L = self.soma.diam = 12.6157\n",
    "        self.dend.L = 200\n",
    "        self.dend.diam = 1\n",
    "\n",
    "    def _setup_biophysics(self):\n",
    "        for sec in self.all:\n",
    "            sec.Ra = 100    # Axial resistance in Ohm * cm\n",
    "            sec.cm = 1      # Membrane capacitance in micro Farads / cm^2\n",
    "        self.soma.insert('hh')                                          \n",
    "        for seg in self.soma:\n",
    "            seg.hh.gnabar = 0.12  # Sodium conductance in S/cm2\n",
    "            seg.hh.gkbar = 0.036  # Potassium conductance in S/cm2\n",
    "            seg.hh.gl = 0.0003    # Leak conductance in S/cm2\n",
    "            seg.hh.el = -54.3     # Reversal potential in mV\n",
    "            \n",
    "        self.dend.insert('pas')                 \n",
    "        for seg in self.dend:\n",
    "            seg.pas.g = 0.001  # Passive conductance in S/cm2\n",
    "            seg.pas.e = -65    # Leak reversal potential mV\n",
    "        self.syn = h.ExpSyn(self.dend(0.5))\n",
    "        self.syn.tau = 2 * ms\n",
    "        self.refrac = 5 * ms\n",
    "        # input이 .syn이라는 한 지역을 통해 입력 되는 것으로 간주시킴.\n",
    "\"\"\"\n",
    "h.ExpSyn decay에 의해 두 개의 ExpSyn object가 같은 point에 있는 것이나 \n",
    "linearly하게 더해지는 서로 다른 두 synapse가 한 군데에 input하는 것과 다름이 없다.\n",
    "이 point를 dend(0.5)로 정하자.\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41dbdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InhibitoryCell(Cell):\n",
    "    name = 'InhibitoryCell'\n",
    "    \n",
    "    def _setup_morphology(self):\n",
    "        self.soma = h.Section(name='soma', cell=self)\n",
    "        self.dend = h.Section(name='dend', cell=self)\n",
    "        self.dend.connect(self.soma)\n",
    "        self.soma.L = self.soma.diam = 12.6157\n",
    "        self.dend.L = 200\n",
    "        self.dend.diam = 1\n",
    "\n",
    "    def _setup_biophysics(self):\n",
    "        for sec in self.all:\n",
    "            sec.Ra = 100    # Axial resistance in Ohm * cm\n",
    "            sec.cm = 1      # Membrane capacitance in micro Farads / cm^2\n",
    "        self.soma.insert('hh')                                          \n",
    "        for seg in self.soma:\n",
    "            seg.hh.gnabar = 0.12  # Sodium conductance in S/cm2\n",
    "            seg.hh.gkbar = 0.036  # Potassium conductance in S/cm2\n",
    "            seg.hh.gl = 0.0003    # Leak conductance in S/cm2\n",
    "            seg.hh.el = -54.3     # Reversal potential in mV\n",
    "            \n",
    "        self.dend.insert('pas')                 \n",
    "        for seg in self.dend:\n",
    "            seg.pas.g = 0.001  # Passive conductance in S/cm2\n",
    "            seg.pas.e = -60    # Leak reversal potential mV\n",
    "        self.syn = h.ExpSyn(self.dend(0.5))\n",
    "        self.syn.tau = 2 * ms\n",
    "        self.refrac = 2*ms\n",
    "        # input이 .syn이라는 한 지역을 통해 입력 되는 것으로 간주시킴.\n",
    "\"\"\"\n",
    "h.ExpSyn decay에 의해 두 개의 ExpSyn object가 같은 point에 있는 것이나 \n",
    "linearly하게 더해지는 서로 다른 두 synapse가 한 군데에 input하는 것과 다름이 없다.\n",
    "이 point를 dend(0.5)로 정하자.\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42dc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExSquare:\n",
    "    def __init__(self, N=5, stim_w=0.04, stim_t=9, stim_delay=1, syn_w=0.01, syn_delay=5, d=25):\n",
    "        self._syn_w = syn_w  #Stimulus weight\n",
    "        self._syn_delay = syn_delay\n",
    "        self._create_cells(N, d)\n",
    "        self._netstim = h.NetStim()\n",
    "        self._netstim.number = 1\n",
    "        self._netstim.start = stim_t\n",
    "        self._nc = h.NetCon(self._netstim, self.cells[0].syn)\n",
    "        self._nc.delay = stim_delay\n",
    "        self._nc.weight[0] = stim_w\n",
    "        \n",
    "    def _create_cells(self, N, d):\n",
    "        self.cells = []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                self.cells.append(ExcitatoryCell(100*i+j, d * i, d * j, 0))\n",
    "    \"\"\"\n",
    "    Ball And Stick #2의 create_n_BallAndStick 함수와 같은 방식으로 세포를 선언/배치.\n",
    "    단, 세포들이 return되는 create_n_BallAndStick과 달리 self.cells에 저장됨.\n",
    "    \"\"\"\n",
    "    def _connect_cells(self, insquare):\n",
    "        for source, target in zip(self.cells, insquare.cells):\n",
    "            nc = h.NetCon(source.soma(0.5)._ref_v, target.syn, sec=source.soma)\n",
    "            nc.weight[0] = self._syn_w\n",
    "            nc.delay = self._syn_delay\n",
    "            source._ncs.append(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6e66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InSquare:\n",
    "    def __init__(self, N=5, stim_w=0.04, stim_t=9, stim_delay=1, syn_w=-0.01, syn_delay=5, d=25):\n",
    "        self._syn_w = syn_w  #Stimulus weight\n",
    "        self._syn_delay = syn_delay\n",
    "        self._create_cells(N, d)\n",
    "        self._netstim = h.NetStim()\n",
    "        self._netstim.number = 1\n",
    "        self._netstim.start = stim_t\n",
    "        self._nc = h.NetCon(self._netstim, self.cells[0].syn)\n",
    "        self._nc.delay = stim_delay\n",
    "        self._nc.weight[0] = stim_w\n",
    "        \n",
    "    def _create_cells(self, N, d):\n",
    "        self.cells = []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                self.cells.append(InhibitoryCell(-100*i-j, d * i, d * j, -50))\n",
    "    \"\"\"\n",
    "    Ball And Stick #2의 create_n_BallAndStick 함수와 같은 방식으로 세포를 선언/배치.\n",
    "    단, 세포들이 return되는 create_n_BallAndStick과 달리 self.cells에 저장됨.\n",
    "    \"\"\"\n",
    "    def _connect_cells(self, exsquare):\n",
    "        \n",
    "        for cell in self.cells:\n",
    "            for excell in exsquare.cells:\n",
    "                if self.cells.index(cell) != exsquare.cells.index(excell):\n",
    "                    source = cell\n",
    "                    target = excell\n",
    "                    nc = h.NetCon(source.soma(0.5)._ref_v, target.syn, sec=source.soma)\n",
    "                    nc.weight[0] = self._syn_w\n",
    "                    nc.delay = self._syn_delay\n",
    "                    source._ncs.append(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5bb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exsquare = ExSquare(N=10)\n",
    "insquare = InSquare(N=10)\n",
    "exsquare._connect_cells(insquare)\n",
    "insquare._connect_cells(exsquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86bb423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(exsquare.cells[0]._ncs))\n",
    "print(len(insquare.cells[0]._ncs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4b1d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_window = h.PlotShape(True)\n",
    "shape_window.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199cc864",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.datasets.mnist' has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11116/1882395826.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#x: n만큼의 random한 mnist 이미지 데이터(n, 784 차원 벡터)(n=2이면 2개의 숫자이미지 & 숫자 하나하나가 784차원)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#y: n만큼의 random한 mnist 이미지 데이터의 label(n, 10 차원 벡터)(n=2이면 2개의 숫자이미지 & 10: 라벨 개수 0~9)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#x(n,284) 차원의 벡터 -> (n, 28, 28) 차원의 벡터로 reshape (이미지화 시키기 위해)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.datasets.mnist' has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "#x: n만큼의 random한 mnist 이미지 데이터(n, 784 차원 벡터)(n=2이면 2개의 숫자이미지 & 숫자 하나하나가 784차원)\n",
    "#y: n만큼의 random한 mnist 이미지 데이터의 label(n, 10 차원 벡터)(n=2이면 2개의 숫자이미지 & 10: 라벨 개수 0~9)\n",
    "x,y = mnist.train.next_batch(n)\n",
    "\n",
    "#x(n,284) 차원의 벡터 -> (n, 28, 28) 차원의 벡터로 reshape (이미지화 시키기 위해)\n",
    "mnist_image = np.array(x).reshape((28,28))\n",
    "plt.title(\"label : \" + str(np.where(y[0] == 1)[0][0]))\n",
    "plt.imshow(mnist_image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7791cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2022 The TensorFlow Datasets Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"MNIST, Fashion MNIST, KMNIST and EMNIST.\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets.public_api as tfds\n",
    "\n",
    "# MNIST constants\n",
    "# CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "_MNIST_URL = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "_MNIST_TRAIN_DATA_FILENAME = \"train-images-idx3-ubyte.gz\"\n",
    "_MNIST_TRAIN_LABELS_FILENAME = \"train-labels-idx1-ubyte.gz\"\n",
    "_MNIST_TEST_DATA_FILENAME = \"t10k-images-idx3-ubyte.gz\"\n",
    "_MNIST_TEST_LABELS_FILENAME = \"t10k-labels-idx1-ubyte.gz\"\n",
    "_MNIST_IMAGE_SIZE = 28\n",
    "MNIST_IMAGE_SHAPE = (_MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, 1)\n",
    "MNIST_NUM_CLASSES = 10\n",
    "_TRAIN_EXAMPLES = 60000\n",
    "_TEST_EXAMPLES = 10000\n",
    "\n",
    "_MNIST_CITATION = \"\"\"\\\n",
    "@article{lecun2010mnist,\n",
    "  title={MNIST handwritten digit database},\n",
    "  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
    "  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
    "  volume={2},\n",
    "  year={2010}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_FASHION_MNIST_CITATION = \"\"\"\\\n",
    "@article{DBLP:journals/corr/abs-1708-07747,\n",
    "  author    = {Han Xiao and\n",
    "               Kashif Rasul and\n",
    "               Roland Vollgraf},\n",
    "  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
    "               Algorithms},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1708.07747},\n",
    "  year      = {2017},\n",
    "  url       = {http://arxiv.org/abs/1708.07747},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1708.07747},\n",
    "  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_K_MNIST_CITATION = \"\"\"\\\n",
    "  @online{clanuwat2018deep,\n",
    "  author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},\n",
    "  title        = {Deep Learning for Classical Japanese Literature},\n",
    "  date         = {2018-12-03},\n",
    "  year         = {2018},\n",
    "  eprintclass  = {cs.CV},\n",
    "  eprinttype   = {arXiv},\n",
    "  eprint       = {cs.CV/1812.01718},\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_EMNIST_CITATION = \"\"\"\\\n",
    "@article{cohen_afshar_tapson_schaik_2017,\n",
    "    title={EMNIST: Extending MNIST to handwritten letters},\n",
    "    DOI={10.1109/ijcnn.2017.7966217},\n",
    "    journal={2017 International Joint Conference on Neural Networks (IJCNN)},\n",
    "    author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},\n",
    "    year={2017}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MNIST(tfds.core.GeneratorBasedBuilder):\n",
    "  \"\"\"MNIST.\"\"\"\n",
    "  URL = _MNIST_URL\n",
    "\n",
    "  VERSION = tfds.core.Version(\"3.0.1\")\n",
    "\n",
    "  def _info(self):\n",
    "    return tfds.core.DatasetInfo(\n",
    "        builder=self,\n",
    "        description=(\"The MNIST database of handwritten digits.\"),\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \"image\": tfds.features.Image(shape=MNIST_IMAGE_SHAPE),\n",
    "            \"label\": tfds.features.ClassLabel(num_classes=MNIST_NUM_CLASSES),\n",
    "        }),\n",
    "        supervised_keys=(\"image\", \"label\"),\n",
    "        homepage=\"http://yann.lecun.com/exdb/mnist/\",\n",
    "        citation=_MNIST_CITATION,\n",
    "    )\n",
    "\n",
    "  def _split_generators(self, dl_manager):\n",
    "    \"\"\"Returns SplitGenerators.\"\"\"\n",
    "    # Download the full MNIST Database\n",
    "    filenames = {\n",
    "        \"train_data\": _MNIST_TRAIN_DATA_FILENAME,\n",
    "        \"train_labels\": _MNIST_TRAIN_LABELS_FILENAME,\n",
    "        \"test_data\": _MNIST_TEST_DATA_FILENAME,\n",
    "        \"test_labels\": _MNIST_TEST_LABELS_FILENAME,\n",
    "    }\n",
    "    mnist_files = dl_manager.download_and_extract(\n",
    "        {k: urllib.parse.urljoin(self.URL, v) for k, v in filenames.items()})\n",
    "\n",
    "    # MNIST provides TRAIN and TEST splits, not a VALIDATION split, so we only\n",
    "    # write the TRAIN and TEST splits to disk.\n",
    "    return [\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TRAIN,\n",
    "            gen_kwargs=dict(\n",
    "                num_examples=_TRAIN_EXAMPLES,\n",
    "                data_path=mnist_files[\"train_data\"],\n",
    "                label_path=mnist_files[\"train_labels\"],\n",
    "            )),\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TEST,\n",
    "            gen_kwargs=dict(\n",
    "                num_examples=_TEST_EXAMPLES,\n",
    "                data_path=mnist_files[\"test_data\"],\n",
    "                label_path=mnist_files[\"test_labels\"],\n",
    "            )),\n",
    "    ]\n",
    "\n",
    "  def _generate_examples(self, num_examples, data_path, label_path):\n",
    "    \"\"\"Generate MNIST examples as dicts.\n",
    "\n",
    "    Args:\n",
    "      num_examples (int): The number of example.\n",
    "      data_path (str): Path to the data files\n",
    "      label_path (str): Path to the labels\n",
    "\n",
    "    Yields:\n",
    "      Generator yielding the next examples\n",
    "    \"\"\"\n",
    "    images = _extract_mnist_images(data_path, num_examples)\n",
    "    labels = _extract_mnist_labels(label_path, num_examples)\n",
    "    data = list(zip(images, labels))\n",
    "\n",
    "    # Using index as key since data is always loaded in same order.\n",
    "    for index, (image, label) in enumerate(data):\n",
    "      record = {\"image\": image, \"label\": label}\n",
    "      yield index, record\n",
    "\n",
    "\n",
    "class FashionMNIST(MNIST):\n",
    "  URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "\n",
    "  # TODO(afrozm): Try to inherit from MNIST's _info and mutate things as needed.\n",
    "  def _info(self):\n",
    "    return tfds.core.DatasetInfo(\n",
    "        builder=self,\n",
    "        description=(\"Fashion-MNIST is a dataset of Zalando's article images \"\n",
    "                     \"consisting of a training set of 60,000 examples and a \"\n",
    "                     \"test set of 10,000 examples. Each example is a 28x28 \"\n",
    "                     \"grayscale image, associated with a label from 10 \"\n",
    "                     \"classes.\"),\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \"image\":\n",
    "                tfds.features.Image(shape=MNIST_IMAGE_SHAPE),\n",
    "            \"label\":\n",
    "                tfds.features.ClassLabel(names=[\n",
    "                    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "                ]),\n",
    "        }),\n",
    "        supervised_keys=(\"image\", \"label\"),\n",
    "        homepage=\"https://github.com/zalandoresearch/fashion-mnist\",\n",
    "        citation=_FASHION_MNIST_CITATION,\n",
    "    )\n",
    "\n",
    "\n",
    "class KMNIST(MNIST):\n",
    "  URL = \"http://codh.rois.ac.jp/kmnist/dataset/kmnist/\"\n",
    "\n",
    "  def _info(self):\n",
    "    return tfds.core.DatasetInfo(\n",
    "        builder=self,\n",
    "        description=(\"Kuzushiji-MNIST is a drop-in replacement for the MNIST \"\n",
    "                     \"dataset (28x28 grayscale, 70,000 images), provided in \"\n",
    "                     \"the original MNIST format as well as a NumPy format. \"\n",
    "                     \"Since MNIST restricts us to 10 classes, we chose one \"\n",
    "                     \"character to represent each of the 10 rows of Hiragana \"\n",
    "                     \"when creating Kuzushiji-MNIST.\"),\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \"image\":\n",
    "                tfds.features.Image(shape=MNIST_IMAGE_SHAPE),\n",
    "            \"label\":\n",
    "                tfds.features.ClassLabel(names=[\n",
    "                    \"o\", \"ki\", \"su\", \"tsu\", \"na\", \"ha\", \"ma\", \"ya\", \"re\", \"wo\"\n",
    "                ]),\n",
    "        }),\n",
    "        supervised_keys=(\"image\", \"label\"),\n",
    "        homepage=\"http://codh.rois.ac.jp/kmnist/index.html.en\",\n",
    "        citation=_K_MNIST_CITATION,\n",
    "    )\n",
    "\n",
    "\n",
    "class EMNISTConfig(tfds.core.BuilderConfig):\n",
    "  \"\"\"BuilderConfig for EMNIST CONFIG.\"\"\"\n",
    "\n",
    "  def __init__(self, *, class_number, train_examples, test_examples, **kwargs):\n",
    "    \"\"\"BuilderConfig for EMNIST class number.\n",
    "\n",
    "    Args:\n",
    "      class_number: There are six different splits provided in this dataset. And\n",
    "        have different class numbers.\n",
    "      train_examples: number of train examples\n",
    "      test_examples: number of test examples\n",
    "      **kwargs: keyword arguments forwarded to super.\n",
    "    \"\"\"\n",
    "    super(EMNISTConfig, self).__init__(**kwargs)\n",
    "    self.class_number = class_number\n",
    "    self.train_examples = train_examples\n",
    "    self.test_examples = test_examples\n",
    "\n",
    "\n",
    "class EMNIST(MNIST):\n",
    "  \"\"\"Emnist dataset.\"\"\"\n",
    "  URL = \"https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip\"\n",
    "  VERSION = tfds.core.Version(\"3.0.0\")\n",
    "  RELEASE_NOTES = {\n",
    "      \"3.0.0\": \"New split API (https://tensorflow.org/datasets/splits)\",\n",
    "  }\n",
    "  BUILDER_CONFIGS = [\n",
    "      EMNISTConfig(\n",
    "          name=\"byclass\",\n",
    "          class_number=62,\n",
    "          train_examples=697932,\n",
    "          test_examples=116323,\n",
    "          description=\"EMNIST ByClass\",\n",
    "      ),\n",
    "      EMNISTConfig(\n",
    "          name=\"bymerge\",\n",
    "          class_number=47,\n",
    "          train_examples=697932,\n",
    "          test_examples=116323,\n",
    "          description=\"EMNIST ByMerge\",\n",
    "      ),\n",
    "      EMNISTConfig(\n",
    "          name=\"balanced\",\n",
    "          class_number=47,\n",
    "          train_examples=112800,\n",
    "          test_examples=18800,\n",
    "          description=\"EMNIST Balanced\",\n",
    "      ),\n",
    "      EMNISTConfig(\n",
    "          name=\"letters\",\n",
    "          class_number=37,\n",
    "          train_examples=88800,\n",
    "          test_examples=14800,\n",
    "          description=\"EMNIST Letters\",\n",
    "      ),\n",
    "      EMNISTConfig(\n",
    "          name=\"digits\",\n",
    "          class_number=10,\n",
    "          train_examples=240000,\n",
    "          test_examples=40000,\n",
    "          description=\"EMNIST Digits\",\n",
    "      ),\n",
    "      EMNISTConfig(\n",
    "          name=\"mnist\",\n",
    "          class_number=10,\n",
    "          train_examples=60000,\n",
    "          test_examples=10000,\n",
    "          description=\"EMNIST MNIST\",\n",
    "      ),\n",
    "  ]\n",
    "\n",
    "  def _info(self):\n",
    "    return tfds.core.DatasetInfo(\n",
    "        builder=self,\n",
    "        description=(\n",
    "            \"The EMNIST dataset is a set of handwritten character digits \"\n",
    "            \"derived from the NIST Special Database 19 and converted to \"\n",
    "            \"a 28x28 pixel image format and dataset structure that directly \"\n",
    "            \"matches the MNIST dataset.\\n\\n\"\n",
    "            \"Note: Like the original EMNIST data, images provided here are \"\n",
    "            \"inverted horizontally and rotated 90 anti-clockwise. You can use \"\n",
    "            \"`tf.transpose` within `ds.map` to convert the images to a \"\n",
    "            \"human-friendlier format.\"),\n",
    "        features=tfds.features.FeaturesDict({\n",
    "            \"image\":\n",
    "                tfds.features.Image(shape=MNIST_IMAGE_SHAPE),\n",
    "            \"label\":\n",
    "                tfds.features.ClassLabel(\n",
    "                    num_classes=self.builder_config.class_number),\n",
    "        }),\n",
    "        supervised_keys=(\"image\", \"label\"),\n",
    "        homepage=(\"https://www.nist.gov/itl/products-and-services/\"\n",
    "                  \"emnist-dataset\"),\n",
    "        citation=_EMNIST_CITATION,\n",
    "    )\n",
    "\n",
    "  def _split_generators(self, dl_manager):\n",
    "    filenames = {\n",
    "        \"train_data\":\n",
    "            \"emnist-{}-train-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name),\n",
    "        \"train_labels\":\n",
    "            \"emnist-{}-train-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name),\n",
    "        \"test_data\":\n",
    "            \"emnist-{}-test-images-idx3-ubyte.gz\".format(\n",
    "                self.builder_config.name),\n",
    "        \"test_labels\":\n",
    "            \"emnist-{}-test-labels-idx1-ubyte.gz\".format(\n",
    "                self.builder_config.name),\n",
    "    }\n",
    "\n",
    "    dir_name = os.path.join(dl_manager.download_and_extract(self.URL), \"gzip\")\n",
    "    extracted = dl_manager.extract(\n",
    "        {k: os.path.join(dir_name, fname) for k, fname in filenames.items()})\n",
    "\n",
    "    return [\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TRAIN,\n",
    "            gen_kwargs=dict(\n",
    "                num_examples=self.builder_config.train_examples,\n",
    "                data_path=extracted[\"train_data\"],\n",
    "                label_path=extracted[\"train_labels\"],\n",
    "            )),\n",
    "        tfds.core.SplitGenerator(\n",
    "            name=tfds.Split.TEST,\n",
    "            gen_kwargs=dict(\n",
    "                num_examples=self.builder_config.test_examples,\n",
    "                data_path=extracted[\"test_data\"],\n",
    "                label_path=extracted[\"test_labels\"],\n",
    "            ))\n",
    "    ]\n",
    "\n",
    "\n",
    "def _extract_mnist_images(image_filepath, num_images):\n",
    "  with tf.io.gfile.GFile(image_filepath, \"rb\") as f:\n",
    "    f.read(16)  # header\n",
    "    buf = f.read(_MNIST_IMAGE_SIZE * _MNIST_IMAGE_SIZE * num_images)\n",
    "    data = np.frombuffer(\n",
    "        buf,\n",
    "        dtype=np.uint8,\n",
    "    ).reshape(num_images, _MNIST_IMAGE_SIZE, _MNIST_IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def _extract_mnist_labels(labels_filepath, num_labels):\n",
    "  with tf.io.gfile.GFile(labels_filepath, \"rb\") as f:\n",
    "    f.read(8)  # header\n",
    "    buf = f.read(num_labels)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd78b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
